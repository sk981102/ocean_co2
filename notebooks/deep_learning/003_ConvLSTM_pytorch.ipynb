{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66801a64-891a-47da-bc03-b88ea5c04182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/xarray/backends/cfgrib_.py:27: UserWarning: Failed to load cfgrib - most likely there is a problem accessing the ecCodes library. Try `import cfgrib` to get the full error message\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a75bb5-5177-4211-90f0-4db4bf61b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1485121/3742897804.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2022-05-31 15:09:38.528396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../../src')\n",
    "from utils import df_to_xarray,read_xarray, get_point_prediction, custom_rmse\n",
    "\n",
    "sys.path.insert(0, '../../src/preprocess')\n",
    "from data_preprocess import preprocess_image_reduced,preprocess_images_nfp, inverse_scale_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2658c449-dbba-41d9-840e-7197fe40b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 3, 180, 360, 6) (418, 3, 180, 360, 1)\n"
     ]
    }
   ],
   "source": [
    "dir_name=\"../../data/data1\"\n",
    "val_dir_name=\"../../data/data2\"\n",
    "\n",
    "data,pco2 = preprocess_images_nfp(dir_name)\n",
    "data_socat, pco2_socat = preprocess_images_nfp(dir_name, socat = True)\n",
    "\n",
    "# val_data,val_pco2 = preprocess_images_nfp(val_dir_name,\"035\")\n",
    "# val_data_socat,val_pco2_socat = preprocess_images_nfp(val_dir_name,\"035\",socat=True)\n",
    "\n",
    "X_index=np.lib.stride_tricks.sliding_window_view(range(421),3) \n",
    "\n",
    "y=np.expand_dims(pco2[X_index][1:],axis=4)\n",
    "X=data[X_index][:-1]\n",
    "\n",
    "# val_y=np.expand_dims(val_pco2[X_index][1:],axis=4)\n",
    "# val_X=val_data[X_index][:-1]\n",
    "\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "INPUT_SHAPE = X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b1ee8d-c423-487d-b80d-f6f1e0b148cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels= 4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        \n",
    "        input_tensor = input_tensor\n",
    "        h_cur = h_cur\n",
    "        \n",
    "        # print(\"input_tensor:\", input_tensor.size())\n",
    "        # print(\"h_cur:\",h_cur.size())\n",
    "        # print(\"c_hur:\",c_cur.size())\n",
    "        # print(\"input_dimension:\",self.input_dim)\n",
    "        # print(\"hidden_dimension:\",self.hidden_dim)\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1) # concatenate along channel axis\n",
    "        #print(\"combined:\", combined.size())\n",
    "        combined_conv = self.conv(combined)\n",
    "        #print(\"combined_conv:\", combined_conv.size())\n",
    "\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = nn.ELU()(cc_g)\n",
    "        \n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * nn.ELU()(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1e14a8-4aac-4c98-b193-c850206f642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderConvLSTM(nn.Module):\n",
    "    def __init__(self, nf, in_chan):\n",
    "        super(EncoderDecoderConvLSTM, self).__init__()\n",
    "\n",
    "        \"\"\" ARCHITECTURE \n",
    "\n",
    "        # Encoder (ConvLSTM)\n",
    "        # Encoder Vector (final hidden state of encoder)\n",
    "        # Decoder (ConvLSTM) - takes Encoder Vector as input\n",
    "        # Decoder (3D CNN) - produces regression predictions for our model\n",
    "\n",
    "        \"\"\"\n",
    "        self.encoder_1_convlstm = ConvLSTMCell(input_dim=in_chan,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.encoder_2_convlstm = ConvLSTMCell(input_dim=nf,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "        \n",
    "        self.batch_norm = torch.nn.BatchNorm2d(nf)\n",
    "\n",
    "        self.decoder_1_convlstm = ConvLSTMCell(input_dim=nf,  # nf + 1\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.decoder_2_convlstm = ConvLSTMCell(input_dim=nf,\n",
    "                                               hidden_dim=nf,\n",
    "                                               kernel_size=(3, 3),\n",
    "                                               bias=True)\n",
    "\n",
    "        self.decoder_CNN = nn.Conv3d(in_channels=nf,\n",
    "                                     out_channels=3,\n",
    "                                     kernel_size=(1, 3, 3),\n",
    "                                     padding=(0, 1, 1))\n",
    "\n",
    "\n",
    "    def autoencoder(self, x, seq_len, future_step, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # encoder\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.encoder_1_convlstm(input_tensor=x[:, t, :, :],\n",
    "                                               cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n",
    "            \n",
    "            h_t2, c_t2 = self.encoder_2_convlstm(input_tensor=h_t,\n",
    "                                                 cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n",
    "\n",
    "        # encoder_vector\n",
    "        encoder_vector = h_t2\n",
    "\n",
    "        # decoder\n",
    "        for t in range(future_step):\n",
    "            h_t3, c_t3 = self.decoder_1_convlstm(input_tensor=encoder_vector,\n",
    "                                                 cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n",
    "            h_t4, c_t4 = self.decoder_2_convlstm(input_tensor=h_t3,\n",
    "                                                 cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n",
    "            encoder_vector = h_t4\n",
    "            outputs += [h_t4]  # predictions\n",
    "\n",
    "        outputs = torch.stack(outputs, 1)\n",
    "        outputs = outputs.permute(0, 2, 1, 3, 4)\n",
    "        outputs = self.decoder_CNN(outputs)\n",
    "        outputs = torch.nn.ELU()(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x, future_seq=0, hidden_state=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor:\n",
    "            5-D Tensor of shape (b, t, c, h, w)        #   batch, time, channel, height, width\n",
    "        \"\"\"\n",
    "\n",
    "        # find size of different input dimensions\n",
    "        b, seq_len, _, h, w = x.size()\n",
    "\n",
    "        # initialize hidden states\n",
    "        h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t3, c_t3 = self.decoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "        h_t4, c_t4 = self.decoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n",
    "\n",
    "        # autoencoder forward\n",
    "        outputs = self.autoencoder(x, seq_len, future_seq, h_t, c_t, h_t2, c_t2, h_t3, c_t3, h_t4, c_t4)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8387de08-b4ed-4ad2-946f-ebf017219dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Checkpoint directory ../../models exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    \"../../models\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True)\n",
    "\n",
    "batch_size = 16\n",
    "lr= 0.0003\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb56d2a4-b23e-4157-a898-7e18500e3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from multiprocessing import Process\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class CESMLighting(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams=None, model=None, X= None, y=None):\n",
    "        super(CESMLighting, self).__init__()\n",
    "        # default config\n",
    "        self.normalize = False\n",
    "        self.model = model\n",
    "\n",
    "        # logging config\n",
    "        self.log_images = True\n",
    "\n",
    "        # Training config\n",
    "        self.batch_size = 16\n",
    "        self.epochs=100\n",
    "        self.n_steps_past = 1\n",
    "        self.n_steps_ahead = 1\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def customRmse(self,y_true, y_pred):\n",
    "        \"\"\"\n",
    "        custom_rmse(y_true, y_pred)\n",
    "        calculates root square mean value with focusing only on the ocean\n",
    "        \"\"\"\n",
    "        y_pred = y_pred[(y_true != 0) & (y_true != 0.0)]\n",
    "        y_true = y_true[(y_true != 0) & (y_true != 0.0)]\n",
    "        \n",
    "        y_pred = torch.Tensor.cuda(y_pred)\n",
    "        y_true = y_true.type(y_pred.dtype)\n",
    "        loss = torch.sqrt(torch.mean(torch.square(torch.subtract(y_pred,y_true))))\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device='cuda')\n",
    "        output = self.model(x, future_seq=self.n_steps_ahead)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        y = y.squeeze().cuda()\n",
    "        y_hat = self.forward(x).squeeze()  \n",
    "        \n",
    "        #print(\"y: {} y_hat: {}\".format(y.shape,y_hat.shape))\n",
    "\n",
    "        loss = self.customRmse(y, y_hat)\n",
    "\n",
    "        # save learning_rate\n",
    "        lr_saved = self.trainer.optimizers[0].param_groups[-1]['lr']\n",
    "        lr_saved = torch.scalar_tensor(lr_saved).cuda()\n",
    "\n",
    "        tensorboard_logs = {'train_rmse_loss': loss,\n",
    "                            'learning_rate': lr_saved}\n",
    "        if self.global_step % 10 == 0:\n",
    "            print(\"Current epoch {} loss: {}\".format(self.current_epoch, loss.item()))\n",
    "        \n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # OPTIONAL\n",
    "        x, y = batch\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        y_hat = self.forward(x)\n",
    "        y = y.squeeze().cuda()\n",
    "        y_hat = self.forward(x).squeeze() \n",
    "        return {'test_loss': self.criterion(y_hat, y)}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(),lr = lr)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        tensor_X = torch.Tensor(self.X) # transform to torch tensor\n",
    "        tensor_y = torch.Tensor(self.y)\n",
    "        cesm = TensorDataset(tensor_X,tensor_y) # create your datset\n",
    "\n",
    "        train_loader = DataLoader(cesm,batch_size=batch_size,shuffle=False) \n",
    "        return train_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        tensor_X = torch.Tensor(self.X) # transform to torch tensor\n",
    "        tensor_y = torch.Tensor(self.y)\n",
    "        cesm = TensorDataset(tensor_X,tensor_y) # create your datset\n",
    "\n",
    "        test_loader = DataLoader(cesm,batch_size=batch_size,shuffle=False) \n",
    "        return test_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672463e1-7d41-402d-839f-ec429c757ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type                   | Params\n",
      "-------------------------------------------------\n",
      "0 | model | EncoderDecoderConvLSTM | 266 K \n",
      "/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feedabedfaa2451297549a06cac88e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 0 loss: 77.02783966064453\n",
      "Current epoch 0 loss: 79.63282775878906\n",
      "Current epoch 0 loss: 79.74320983886719\n",
      "Current epoch 0 loss: 57.55128860473633\n",
      "Current epoch 0 loss: 24.480430603027344\n",
      "Current epoch 0 loss: 16.296785354614258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: loss reached 15.91725 (best 15.91725), saving model to ../../models/epoch=0.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 1 loss: 19.806032180786133\n",
      "Current epoch 1 loss: 13.998576164245605\n",
      "Current epoch 1 loss: 12.664587020874023\n",
      "Current epoch 1 loss: 12.021626472473145\n",
      "Current epoch 1 loss: 11.473539352416992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: loss reached 12.49705 (best 12.49705), saving model to ../../models/epoch=1.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 2 loss: 21.973155975341797\n",
      "Current epoch 2 loss: 12.41552734375\n",
      "Current epoch 2 loss: 11.047698020935059\n",
      "Current epoch 2 loss: 12.335343360900879\n",
      "Current epoch 2 loss: 11.971892356872559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: loss reached 11.06072 (best 11.06072), saving model to ../../models/epoch=2.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 2 loss: 11.060720443725586\n",
      "Current epoch 3 loss: 12.800847053527832\n",
      "Current epoch 3 loss: 11.979772567749023\n",
      "Current epoch 3 loss: 11.38894271850586\n",
      "Current epoch 3 loss: 10.439363479614258\n",
      "Current epoch 3 loss: 12.33543586730957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: loss reached 10.50767 (best 10.50767), saving model to ../../models/epoch=3.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 4 loss: 15.611891746520996\n",
      "Current epoch 4 loss: 12.915993690490723\n",
      "Current epoch 4 loss: 11.577649116516113\n",
      "Current epoch 4 loss: 9.777384757995605\n",
      "Current epoch 4 loss: 10.01101016998291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: loss reached 9.91624 (best 9.91624), saving model to ../../models/epoch=4.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 5 loss: 13.112822532653809\n",
      "Current epoch 5 loss: 10.136364936828613\n",
      "Current epoch 5 loss: 8.221946716308594\n",
      "Current epoch 5 loss: 9.056938171386719\n",
      "Current epoch 5 loss: 8.926743507385254\n",
      "Current epoch 5 loss: 9.070084571838379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: loss reached 9.24798 (best 9.24798), saving model to ../../models/epoch=5.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 6 loss: 8.369848251342773\n",
      "Current epoch 6 loss: 9.246298789978027\n",
      "Current epoch 6 loss: 9.577816009521484\n",
      "Current epoch 6 loss: 8.127857208251953\n",
      "Current epoch 6 loss: 8.659805297851562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: loss reached 8.45080 (best 8.45080), saving model to ../../models/epoch=6.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 7 loss: 10.993430137634277\n",
      "Current epoch 7 loss: 10.898250579833984\n",
      "Current epoch 7 loss: 10.066965103149414\n",
      "Current epoch 7 loss: 7.9469804763793945\n",
      "Current epoch 7 loss: 9.129315376281738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: loss reached 8.63018 (best 8.45080), saving model to ../../models/epoch=7.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 7 loss: 8.630182266235352\n",
      "Current epoch 8 loss: 7.314732074737549\n",
      "Current epoch 8 loss: 7.682033538818359\n",
      "Current epoch 8 loss: 7.190796852111816\n",
      "Current epoch 8 loss: 7.248157024383545\n",
      "Current epoch 8 loss: 7.866811275482178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: loss reached 7.71010 (best 7.71010), saving model to ../../models/epoch=8.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 9 loss: 7.413439750671387\n",
      "Current epoch 9 loss: 6.698433876037598\n",
      "Current epoch 9 loss: 7.198547840118408\n",
      "Current epoch 9 loss: 7.007115364074707\n",
      "Current epoch 9 loss: 6.971301078796387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: loss reached 7.65654 (best 7.65654), saving model to ../../models/epoch=9.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 10 loss: 7.354216575622559\n",
      "Current epoch 10 loss: 6.863980770111084\n",
      "Current epoch 10 loss: 6.6054582595825195\n",
      "Current epoch 10 loss: 7.089925765991211\n",
      "Current epoch 10 loss: 6.842605113983154\n",
      "Current epoch 10 loss: 6.895432949066162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: loss reached 7.39292 (best 7.39292), saving model to ../../models/epoch=10.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 11 loss: 8.349703788757324\n",
      "Current epoch 11 loss: 6.62127685546875\n",
      "Current epoch 11 loss: 6.462490081787109\n",
      "Current epoch 11 loss: 7.434760093688965\n",
      "Current epoch 11 loss: 7.1342620849609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: loss  was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 12 loss: 6.7259907722473145\n",
      "Current epoch 12 loss: 6.808413505554199\n",
      "Current epoch 12 loss: 6.531578540802002\n",
      "Current epoch 12 loss: 6.721225738525391\n",
      "Current epoch 12 loss: 7.474700927734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: loss reached 7.21664 (best 7.21664), saving model to ../../models/epoch=12.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 12 loss: 7.216640472412109\n",
      "Current epoch 13 loss: 6.307109355926514\n",
      "Current epoch 13 loss: 6.680541038513184\n",
      "Current epoch 13 loss: 6.426871299743652\n",
      "Current epoch 13 loss: 6.414636611938477\n",
      "Current epoch 13 loss: 7.087423801422119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: loss  was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 14 loss: 6.437928199768066\n",
      "Current epoch 14 loss: 6.0503034591674805\n",
      "Current epoch 14 loss: 6.535053730010986\n",
      "Current epoch 14 loss: 6.315428256988525\n",
      "Current epoch 14 loss: 6.3279547691345215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00014: loss reached 6.86223 (best 6.86223), saving model to ../../models/epoch=14.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 15 loss: 9.83947467803955\n",
      "Current epoch 15 loss: 6.6800689697265625\n",
      "Current epoch 15 loss: 6.407829761505127\n",
      "Current epoch 15 loss: 6.543723106384277\n",
      "Current epoch 15 loss: 6.655492305755615\n",
      "Current epoch 15 loss: 6.479568958282471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: loss  was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 16 loss: 6.495143413543701\n",
      "Current epoch 16 loss: 6.256110191345215\n",
      "Current epoch 16 loss: 6.121934413909912\n",
      "Current epoch 16 loss: 6.617790699005127\n",
      "Current epoch 16 loss: 6.258050441741943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: loss reached 6.67916 (best 6.67916), saving model to ../../models/epoch=16.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 17 loss: 5.936398506164551\n",
      "Current epoch 17 loss: 6.232260227203369\n",
      "Current epoch 17 loss: 5.890419960021973\n",
      "Current epoch 17 loss: 5.833022594451904\n",
      "Current epoch 17 loss: 6.564389705657959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: loss reached 6.38574 (best 6.38574), saving model to ../../models/epoch=17.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 17 loss: 6.385737895965576\n",
      "Current epoch 18 loss: 5.38316535949707\n",
      "Current epoch 18 loss: 5.833261966705322\n",
      "Current epoch 18 loss: 5.439970016479492\n",
      "Current epoch 18 loss: 5.968562126159668\n",
      "Current epoch 18 loss: 6.652337551116943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: loss  was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 19 loss: 7.481805324554443\n",
      "Current epoch 19 loss: 5.355412006378174\n",
      "Current epoch 19 loss: 5.7155351638793945\n",
      "Current epoch 19 loss: 5.573824882507324\n",
      "Current epoch 19 loss: 5.631882667541504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: loss reached 5.59751 (best 5.59751), saving model to ../../models/epoch=19.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 20 loss: 9.748916625976562\n",
      "Current epoch 20 loss: 6.480207920074463\n",
      "Current epoch 20 loss: 6.205139636993408\n",
      "Current epoch 20 loss: 6.2918901443481445\n",
      "Current epoch 20 loss: 5.404109001159668\n",
      "Current epoch 20 loss: 5.244962215423584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: loss reached 5.68052 (best 5.59751), saving model to ../../models/epoch=20.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 21 loss: 8.617737770080566\n",
      "Current epoch 21 loss: 5.186371326446533\n",
      "Current epoch 21 loss: 4.982203960418701\n",
      "Current epoch 21 loss: 5.077139854431152\n",
      "Current epoch 21 loss: 5.630277633666992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: loss reached 5.19990 (best 5.19990), saving model to ../../models/epoch=21.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 22 loss: 4.783489227294922\n",
      "Current epoch 22 loss: 6.319832801818848\n",
      "Current epoch 22 loss: 5.917200088500977\n",
      "Current epoch 22 loss: 5.520890235900879\n",
      "Current epoch 22 loss: 5.173210620880127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00022: loss  was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 22 loss: 5.684325695037842\n",
      "Current epoch 23 loss: 9.59655475616455\n",
      "Current epoch 23 loss: 5.119865417480469\n",
      "Current epoch 23 loss: 7.115817070007324\n",
      "Current epoch 23 loss: 6.105560779571533\n",
      "Current epoch 23 loss: 5.560757637023926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: loss reached 5.51364 (best 5.19990), saving model to ../../models/epoch=23.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 24 loss: 5.084414482116699\n",
      "Current epoch 24 loss: 5.826329708099365\n",
      "Current epoch 24 loss: 5.375465393066406\n",
      "Current epoch 24 loss: 5.209454536437988\n",
      "Current epoch 24 loss: 4.528156757354736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: loss reached 4.98946 (best 4.98946), saving model to ../../models/epoch=24.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 25 loss: 7.0077409744262695\n",
      "Current epoch 25 loss: 5.300832748413086\n",
      "Current epoch 25 loss: 4.884790897369385\n",
      "Current epoch 25 loss: 4.76303243637085\n",
      "Current epoch 25 loss: 4.73054838180542\n",
      "Current epoch 25 loss: 4.7788004875183105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: loss reached 5.01986 (best 4.98946), saving model to ../../models/epoch=25.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 26 loss: 6.848832607269287\n",
      "Current epoch 26 loss: 5.382123947143555\n",
      "Current epoch 26 loss: 4.258955001831055\n",
      "Current epoch 26 loss: 4.698692798614502\n",
      "Current epoch 26 loss: 4.581749439239502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: loss  was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 27 loss: 5.365386486053467\n",
      "Current epoch 27 loss: 5.861317157745361\n",
      "Current epoch 27 loss: 4.721887111663818\n",
      "Current epoch 27 loss: 4.674215793609619\n",
      "Current epoch 27 loss: 4.785220146179199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: loss reached 4.81152 (best 4.81152), saving model to ../../models/epoch=27.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 27 loss: 4.811515808105469\n",
      "Current epoch 28 loss: 4.124114513397217\n",
      "Current epoch 28 loss: 4.217257022857666\n",
      "Current epoch 28 loss: 3.9831297397613525\n",
      "Current epoch 28 loss: 4.225827693939209\n",
      "Current epoch 28 loss: 4.642294883728027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: loss reached 4.63099 (best 4.63099), saving model to ../../models/epoch=28_v0.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 29 loss: 4.638250827789307\n",
      "Current epoch 29 loss: 4.3092217445373535\n",
      "Current epoch 29 loss: 4.108780384063721\n",
      "Current epoch 29 loss: 4.1348772048950195\n",
      "Current epoch 29 loss: 4.0517754554748535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: loss reached 4.47527 (best 4.47527), saving model to ../../models/epoch=29_v0.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 30 loss: 6.928339004516602\n",
      "Current epoch 30 loss: 5.3869171142578125\n",
      "Current epoch 30 loss: 4.092748165130615\n",
      "Current epoch 30 loss: 4.67457389831543\n",
      "Current epoch 30 loss: 4.5343804359436035\n",
      "Current epoch 30 loss: 4.555964469909668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: loss reached 4.60383 (best 4.47527), saving model to ../../models/epoch=30.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 31 loss: 6.018604278564453\n",
      "Current epoch 31 loss: 4.7108988761901855\n",
      "Current epoch 31 loss: 4.130916118621826\n",
      "Current epoch 31 loss: 4.5749711990356445\n",
      "Current epoch 31 loss: 3.9530558586120605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: loss reached 4.50015 (best 4.47527), saving model to ../../models/epoch=31_v0.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 32 loss: 4.54709005355835\n",
      "Current epoch 32 loss: 5.141644477844238\n",
      "Current epoch 32 loss: 4.771138668060303\n",
      "Current epoch 32 loss: 4.258011817932129\n",
      "Current epoch 32 loss: 4.168031215667725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: loss reached 4.31644 (best 4.31644), saving model to ../../models/epoch=32_v0.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 32 loss: 4.316435813903809\n",
      "Current epoch 33 loss: 3.9482688903808594\n",
      "Current epoch 33 loss: 3.87135910987854\n",
      "Current epoch 33 loss: 3.902240514755249\n",
      "Current epoch 33 loss: 4.003685474395752\n",
      "Current epoch 33 loss: 4.30327033996582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: loss reached 4.31168 (best 4.31168), saving model to ../../models/epoch=33.ckpt as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch 34 loss: 4.266963481903076\n",
      "Current epoch 34 loss: 4.3668365478515625\n",
      "Current epoch 34 loss: 4.030972003936768\n",
      "Current epoch 34 loss: 4.083408832550049\n",
      "Current epoch 34 loss: 3.8343653678894043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: loss  was not in top 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute '__module__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m35\u001b[39m,checkpoint_callback\u001b[38;5;241m=\u001b[39mcheckpoint_callback)\n\u001b[1;32m     10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model)\n\u001b[0;32m---> 12\u001b[0m model_scripted \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model_scripted\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../models/ConvLSTM_pytorch.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/torch/jit/_script.py:897\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m qualified_name \u001b[38;5;241m=\u001b[39m _qualified_name(obj)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;66;03m# If this type is a `nn.Module` subclass, they probably meant to pass\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# an instance instead of a Module\u001b[39;00m\n",
      "File \u001b[0;32m/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/torch/jit/_recursive.py:351\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nn_module, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mRecursiveScriptModule)\n\u001b[1;32m    350\u001b[0m check_module_initialized(nn_module)\n\u001b[0;32m--> 351\u001b[0m concrete_type \u001b[38;5;241m=\u001b[39m \u001b[43mget_module_concrete_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshare_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/torch/jit/_recursive.py:327\u001b[0m, in \u001b[0;36mget_module_concrete_type\u001b[0;34m(nn_module, share_types)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn_module\u001b[38;5;241m.\u001b[39m_concrete_type\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m share_types:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# Look into the store of cached JIT types\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     concrete_type \u001b[38;5;241m=\u001b[39m \u001b[43mconcrete_type_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_concrete_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# Get a concrete type directly, without trying to re-use an existing JIT\u001b[39;00m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# type from the type store.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     concrete_type_builder \u001b[38;5;241m=\u001b[39m infer_concrete_type_builder(nn_module, share_types)\n",
      "File \u001b[0;32m/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/torch/jit/_recursive.py:276\u001b[0m, in \u001b[0;36mConcreteTypeStore.get_or_create_concrete_type\u001b[0;34m(self, nn_module)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_concrete_type\u001b[39m(\u001b[38;5;28mself\u001b[39m, nn_module):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    Infer a ConcreteType from this `nn.Module` instance. Underlying JIT\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    types are re-used if possible.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     concrete_type_builder \u001b[38;5;241m=\u001b[39m \u001b[43minfer_concrete_type_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     nn_module_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(nn_module)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nn_module_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_store:\n",
      "File \u001b[0;32m/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/torch/jit/_recursive.py:165\u001b[0m, in \u001b[0;36minfer_concrete_type_builder\u001b[0;34m(nn_module, share_types)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Constants annotated via `Final[T]` rather than being added to `__constants__`\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, ann \u001b[38;5;129;01min\u001b[39;00m class_annotations\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    166\u001b[0m         constants_set\u001b[38;5;241m.\u001b[39madd(name)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m constants_set:\n",
      "File \u001b[0;32m/burg/glab/users/sk4973/venv2/lib/python3.8/site-packages/torch/_jit_internal.py:735\u001b[0m, in \u001b[0;36mis_final\u001b[0;34m(ann)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_final\u001b[39m(ann):\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mann\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtyping_extensions\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    736\u001b[0m         (\u001b[38;5;28mgetattr\u001b[39m(ann, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__origin__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m Final)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute '__module__'"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "conv_lstm_model = EncoderDecoderConvLSTM(nf=32, in_chan=6)\n",
    "model = CESMLighting(model=conv_lstm_model,X=X,y=y)    \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "trainer = Trainer(max_epochs=35,checkpoint_callback=checkpoint_callback)\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d40c9ea-ebea-4dc0-8756-d6cf295e54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH ='../../models/pytorch_convlstm'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af9d1d5-dc0e-476c-b894-f1d8b05fe82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CESMLighting(\n",
       "  (model): EncoderDecoderConvLSTM(\n",
       "    (encoder_1_convlstm): ConvLSTMCell(\n",
       "      (conv): Conv2d(38, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (encoder_2_convlstm): ConvLSTMCell(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (decoder_1_convlstm): ConvLSTMCell(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder_2_convlstm): ConvLSTMCell(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder_CNN): Conv3d(32, 3, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_lstm_model = EncoderDecoderConvLSTM(nf=32, in_chan=6)\n",
    "model = CESMLighting(model=conv_lstm_model,X=X,y=y)   \n",
    "model.load_state_dict(torch.load(PATH))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d45113f7-a4bc-46a3-ba65-5ad490f31da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_X= torch.Tensor(X).cuda()\n",
    "tensor_y = torch.Tensor(y).cuda()\n",
    "cesm = TensorDataset(tensor_X,tensor_y) # create your datset\n",
    "\n",
    "data_loader = DataLoader(cesm,batch_size=1,shuffle=False) \n",
    "\n",
    "y_pred=np.zeros(shape=([418,3,180, 360,1]))\n",
    "\n",
    "for batch_idx, samples in enumerate(data_loader):\n",
    "    s= samples[0]\n",
    "    s = s.permute(0, 1, 4, 2, 3)\n",
    "    #print(batch_idx, s.size())\n",
    "    yp = model(s)\n",
    "    yp = yp.permute(0,1,3,4,2)\n",
    "    yp= yp.cpu().detach().numpy()\n",
    "    y_pred[batch_idx]=yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6fca4a5-46b9-4598-9c45-9065c8e58bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y==0]=0.0\n",
    "pred=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bf9b42b-3994-405a-ab57-d4c8325f0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "chl,mld,sss,sst,u10,xco2,icefrac,patm,pco2t2 = read_xarray(dir_name)\n",
    "\n",
    "y_true,y_pred = inverse_scale_frame(pred,pco2t2.pCO2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a7f9d5e-83e5-4bec-a4a5-2f91cfffbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_socat = np.copy(y_pred)\n",
    "y_true_socat=np.expand_dims(pco2t2.pCO2_socat.data[X_index][1:],axis=4)\n",
    "y_true_socat = np.nan_to_num(y_true_socat)\n",
    "y_pred_socat[y_true_socat==0]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eefb26fe-cc29-497a-9061-9ced5b0e7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full RMSE score:\n",
      "8.116834158885355\n",
      "SOCAT RMSE score:\n",
      "9.008042187833174\n"
     ]
    }
   ],
   "source": [
    "print(\"Full RMSE score:\")\n",
    "a=custom_rmse(y_pred[:,:1],y_true[:,:1])\n",
    "print(a.numpy())\n",
    "\n",
    "print(\"SOCAT RMSE score:\")\n",
    "b=custom_rmse(y_pred_socat[:,:1],y_true_socat[:,:1])\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4084e6-7603-4a56-8669-ed90f47d132b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
